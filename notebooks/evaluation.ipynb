{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete SQL Evaluation with Full Spider Metrics\n",
    "\n",
    "**Comprehensive evaluation including:**\n",
    "1. String-based metrics (exact match, keyword F1)\n",
    "2. Execution accuracy (database result matching)\n",
    "3. **Full Spider component-wise evaluation** (SELECT, WHERE, GROUP BY, ORDER BY, HAVING, AND/OR, IUEN, keywords)\n",
    "4. Accuracy, Recall, and F1 scores for each SQL component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/aswat/OneDrive - UWA/Desktop/sem4/CITS5553/explainable-nl-query-db-agents')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()  # Current notebook location\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebooks' else NOTEBOOK_DIR\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/aswat/OneDrive - UWA/Desktop/sem4/CITS5553/explainable-nl-query-db-agents/SQL_Prediction_Result.txt')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "FILE_PATH = PROJECT_ROOT/\"SQL_Prediction_Result.txt\"\n",
    "DB_DIR = PROJECT_ROOT/\"notebooks\"/\"database\"\n",
    "\n",
    "USE_EXEC = True\n",
    "SHOW_FIRST_N_MISMATCHES = 10\n",
    "FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLTK available\n",
      "✓ Imports and constants loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Try to import nltk\n",
    "try:\n",
    "    from nltk import word_tokenize\n",
    "    print(\"✓ NLTK available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ NLTK not available, using simple tokenization\")\n",
    "    def word_tokenize(s):\n",
    "        return s.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ').split()\n",
    "\n",
    "# SQL Constants from process_sql.py\n",
    "CLAUSE_KEYWORDS = ('select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', 'except')\n",
    "JOIN_KEYWORDS = ('join', 'on', 'as')\n",
    "WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
    "UNIT_OPS = ('none', '-', '+', \"*\", '/')\n",
    "AGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
    "TABLE_TYPE = {'sql': \"sql\", 'table_unit': \"table_unit\"}\n",
    "COND_OPS = ('and', 'or')\n",
    "SQL_OPS = ('intersect', 'union', 'except')\n",
    "ORDER_OPS = ('desc', 'asc')\n",
    "\n",
    "SQL_KEYWORDS = {\n",
    "    'select', 'from', 'where', 'group', 'order', 'by', 'having', 'limit',\n",
    "    'join', 'inner', 'left', 'right', 'full', 'outer', 'on', 'as',\n",
    "    'union', 'intersect', 'except', 'and', 'or', 'not', 'in', 'like',\n",
    "    'exists', 'between', 'asc', 'desc', 'distinct', 'count', 'sum',\n",
    "    'avg', 'max', 'min'\n",
    "}\n",
    "\n",
    "print(\"✓ Imports and constants loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process SQL - Full Parser Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schema and tokenization functions loaded\n"
     ]
    }
   ],
   "source": [
    "class Schema:\n",
    "    \"\"\"Simple schema which maps table&column to a unique identifier\"\"\"\n",
    "    def __init__(self, schema):\n",
    "        self._schema = schema\n",
    "        self._idMap = self._map(self._schema)\n",
    "\n",
    "    @property\n",
    "    def schema(self):\n",
    "        return self._schema\n",
    "\n",
    "    @property\n",
    "    def idMap(self):\n",
    "        return self._idMap\n",
    "\n",
    "    def _map(self, schema):\n",
    "        idMap = {'*': \"__all__\"}\n",
    "        id = 1\n",
    "        for key, vals in schema.items():\n",
    "            for val in vals:\n",
    "                idMap[key.lower() + \".\" + val.lower()] = \"__\" + key.lower() + \".\" + val.lower() + \"__\"\n",
    "                id += 1\n",
    "        for key in schema:\n",
    "            idMap[key.lower()] = \"__\" + key.lower() + \"__\"\n",
    "            id += 1\n",
    "        return idMap\n",
    "\n",
    "\n",
    "def get_schema(db):\n",
    "    \"\"\"Get database's schema\"\"\"\n",
    "    schema = {}\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [str(table[0].lower()) for table in cursor.fetchall()]\n",
    "    for table in tables:\n",
    "        cursor.execute(\"PRAGMA table_info({})\".format(table))\n",
    "        schema[table] = [str(col[1].lower()) for col in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return schema\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"Robust SQL tokenizer that doesn't rely on NLTK\"\"\"\n",
    "    string = str(string)\n",
    "    string = string.replace(\"'\", '\"')  # Normalize quotes\n",
    "    \n",
    "    # Extract quoted strings first\n",
    "    quote_idxs = [idx for idx, char in enumerate(string) if char == '\"']\n",
    "    \n",
    "    vals = {}\n",
    "    # Only process if we have matching pairs\n",
    "    if len(quote_idxs) % 2 == 0:\n",
    "        for i in range(len(quote_idxs)-1, -1, -2):\n",
    "            qidx1 = quote_idxs[i-1]\n",
    "            qidx2 = quote_idxs[i]\n",
    "            val = string[qidx1: qidx2+1]\n",
    "            key = \"__val_{}_{}__\".format(qidx1, qidx2)\n",
    "            string = string[:qidx1] + key + string[qidx2+1:]\n",
    "            vals[key] = val\n",
    "    \n",
    "    # Simple but effective tokenization\n",
    "    # Add spaces around operators and parentheses\n",
    "    string = string.replace('(', ' ( ')\n",
    "    string = string.replace(')', ' ) ')\n",
    "    string = string.replace(',', ' , ')\n",
    "    string = string.replace(';', ' ; ')\n",
    "    string = string.replace('=', ' = ')\n",
    "    string = string.replace('>', ' > ')\n",
    "    string = string.replace('<', ' < ')\n",
    "    string = string.replace('!', ' ! ')\n",
    "    string = string.replace('+', ' + ')\n",
    "    string = string.replace('-', ' - ')\n",
    "    string = string.replace('*', ' * ')\n",
    "    string = string.replace('/', ' / ')\n",
    "    \n",
    "    # Tokenize by whitespace\n",
    "    toks = [word.lower() for word in string.split() if word.strip()]\n",
    "    \n",
    "    # Replace placeholders with original quoted values\n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] in vals:\n",
    "            toks[i] = vals[toks[i]]\n",
    "    \n",
    "    # Fix !=, >=, <= that got split\n",
    "    fixed_toks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        if i + 1 < len(toks):\n",
    "            if toks[i] == '!' and toks[i+1] == '=':\n",
    "                fixed_toks.append('!=')\n",
    "                i += 2\n",
    "                continue\n",
    "            elif toks[i] == '>' and toks[i+1] == '=':\n",
    "                fixed_toks.append('>=')\n",
    "                i += 2\n",
    "                continue\n",
    "            elif toks[i] == '<' and toks[i+1] == '=':\n",
    "                fixed_toks.append('<=')\n",
    "                i += 2\n",
    "                continue\n",
    "        fixed_toks.append(toks[i])\n",
    "        i += 1\n",
    "    \n",
    "    return fixed_toks\n",
    "\n",
    "\n",
    "def scan_alias(toks):\n",
    "    as_idxs = [idx for idx, tok in enumerate(toks) if tok == 'as']\n",
    "    alias = {}\n",
    "    for idx in as_idxs:\n",
    "        if idx + 1 < len(toks):\n",
    "            alias[toks[idx+1]] = toks[idx-1]\n",
    "    return alias\n",
    "\n",
    "\n",
    "def get_tables_with_alias(schema, toks):\n",
    "    tables = scan_alias(toks)\n",
    "    for key in schema:\n",
    "        tables[key] = key\n",
    "    return tables\n",
    "\n",
    "\n",
    "def skip_semicolon(toks, start_idx):\n",
    "    idx = start_idx\n",
    "    while idx < len(toks) and toks[idx] == \";\":\n",
    "        idx += 1\n",
    "    return idx\n",
    "\n",
    "print(\"✓ Schema and tokenization functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Column and value parsing functions loaded\n"
     ]
    }
   ],
   "source": [
    "def parse_col(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    tok = toks[start_idx]\n",
    "    if tok == \"*\":\n",
    "        return start_idx + 1, schema.idMap[tok]\n",
    "\n",
    "    if '.' in tok:\n",
    "        alias, col = tok.split('.')\n",
    "        key = tables_with_alias[alias] + \".\" + col\n",
    "        return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    if default_tables is None or len(default_tables) == 0:\n",
    "        return start_idx+1, tok\n",
    "\n",
    "    for alias in default_tables:\n",
    "        table = tables_with_alias[alias]\n",
    "        if tok in schema.schema[table]:\n",
    "            key = table + \".\" + tok\n",
    "            return start_idx+1, schema.idMap[key]\n",
    "\n",
    "    return start_idx+1, tok\n",
    "\n",
    "\n",
    "def parse_col_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    isDistinct = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if toks[idx] in AGG_OPS:\n",
    "        agg_id = AGG_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        if idx < len_ and toks[idx] == '(':\n",
    "            idx += 1\n",
    "        if idx < len_ and toks[idx] == \"distinct\":\n",
    "            idx += 1\n",
    "            isDistinct = True\n",
    "        idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        if idx < len_ and toks[idx] == ')':\n",
    "            idx += 1\n",
    "        return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "    if toks[idx] == \"distinct\":\n",
    "        idx += 1\n",
    "        isDistinct = True\n",
    "    agg_id = AGG_OPS.index(\"none\")\n",
    "    idx, col_id = parse_col(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        if idx < len_ and toks[idx] == ')':\n",
    "            idx += 1\n",
    "\n",
    "    return idx, (agg_id, col_id, isDistinct)\n",
    "\n",
    "\n",
    "def parse_val_unit(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    col_unit1 = None\n",
    "    col_unit2 = None\n",
    "    unit_op = UNIT_OPS.index('none')\n",
    "\n",
    "    idx, col_unit1 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    if idx < len_ and toks[idx] in UNIT_OPS:\n",
    "        unit_op = UNIT_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        idx, col_unit2 = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "\n",
    "    if isBlock:\n",
    "        if idx < len_ and toks[idx] == ')':\n",
    "            idx += 1\n",
    "\n",
    "    return idx, (unit_op, col_unit1, col_unit2)\n",
    "\n",
    "\n",
    "def parse_table_unit(toks, start_idx, tables_with_alias, schema):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    key = tables_with_alias.get(toks[idx], toks[idx])\n",
    "\n",
    "    if idx + 1 < len_ and toks[idx+1] == \"as\":\n",
    "        idx += 3\n",
    "    else:\n",
    "        idx += 1\n",
    "\n",
    "    return idx, schema.idMap.get(key, key), key\n",
    "\n",
    "\n",
    "def parse_value(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    isBlock = False\n",
    "    if toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    if idx < len_ and toks[idx] == 'select':\n",
    "        idx, val = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "    elif idx < len_ and \"\\\"\" in toks[idx]:\n",
    "        val = toks[idx]\n",
    "        idx += 1\n",
    "    else:\n",
    "        try:\n",
    "            val = float(toks[idx])\n",
    "            idx += 1\n",
    "        except:\n",
    "            end_idx = idx\n",
    "            while end_idx < len_ and toks[end_idx] != ',' and toks[end_idx] != ')' \\\n",
    "                and toks[end_idx] != 'and' and toks[end_idx] not in CLAUSE_KEYWORDS and toks[end_idx] not in JOIN_KEYWORDS:\n",
    "                    end_idx += 1\n",
    "\n",
    "            idx, val = parse_col_unit(toks[start_idx: end_idx], 0, tables_with_alias, schema, default_tables)\n",
    "            idx = end_idx\n",
    "\n",
    "    if isBlock:\n",
    "        if idx < len_ and toks[idx] == ')':\n",
    "            idx += 1\n",
    "\n",
    "    return idx, val\n",
    "\n",
    "print(\"✓ Column and value parsing functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Condition and FROM clause parsing loaded\n"
     ]
    }
   ],
   "source": [
    "def parse_condition(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        not_op = False\n",
    "        if idx < len_ and toks[idx] == 'not':\n",
    "            not_op = True\n",
    "            idx += 1\n",
    "\n",
    "        if idx >= len_ or toks[idx] not in WHERE_OPS:\n",
    "            break\n",
    "            \n",
    "        op_id = WHERE_OPS.index(toks[idx])\n",
    "        idx += 1\n",
    "        val1 = val2 = None\n",
    "        if op_id == WHERE_OPS.index('between'):\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            if idx < len_ and toks[idx] == 'and':\n",
    "                idx += 1\n",
    "            idx, val2 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        else:\n",
    "            idx, val1 = parse_value(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            val2 = None\n",
    "\n",
    "        conds.append((not_op, op_id, val_unit, val1, val2))\n",
    "\n",
    "        if idx < len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\") or toks[idx] in JOIN_KEYWORDS):\n",
    "            break\n",
    "\n",
    "        if idx < len_ and toks[idx] in COND_OPS:\n",
    "            conds.append(toks[idx])\n",
    "            idx += 1\n",
    "\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_select(toks, start_idx, tables_with_alias, schema, default_tables=None):\n",
    "    idx = start_idx\n",
    "    len_ = len(toks)\n",
    "\n",
    "    if idx >= len_ or toks[idx] != 'select':\n",
    "        return idx, (False, [])\n",
    "        \n",
    "    idx += 1\n",
    "    isDistinct = False\n",
    "    if idx < len_ and toks[idx] == 'distinct':\n",
    "        idx += 1\n",
    "        isDistinct = True\n",
    "    val_units = []\n",
    "\n",
    "    while idx < len_ and toks[idx] not in CLAUSE_KEYWORDS:\n",
    "        agg_id = AGG_OPS.index(\"none\")\n",
    "        if toks[idx] in AGG_OPS:\n",
    "            agg_id = AGG_OPS.index(toks[idx])\n",
    "            idx += 1\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        val_units.append((agg_id, val_unit))\n",
    "        if idx < len_ and toks[idx] == ',':\n",
    "            idx += 1\n",
    "\n",
    "    return idx, (isDistinct, val_units)\n",
    "\n",
    "\n",
    "def parse_from(toks, start_idx, tables_with_alias, schema):\n",
    "    len_ = len(toks)\n",
    "    \n",
    "    if 'from' not in toks[start_idx:]:\n",
    "        return start_idx, [], [], []\n",
    "    \n",
    "    idx = toks.index('from', start_idx) + 1\n",
    "    default_tables = []\n",
    "    table_units = []\n",
    "    conds = []\n",
    "\n",
    "    while idx < len_:\n",
    "        isBlock = False\n",
    "        if toks[idx] == '(':\n",
    "            isBlock = True\n",
    "            idx += 1\n",
    "\n",
    "        if idx < len_ and toks[idx] == 'select':\n",
    "            idx, sql = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['sql'], sql))\n",
    "        else:\n",
    "            if idx < len_ and toks[idx] == 'join':\n",
    "                idx += 1\n",
    "            idx, table_unit, table_name = parse_table_unit(toks, idx, tables_with_alias, schema)\n",
    "            table_units.append((TABLE_TYPE['table_unit'],table_unit))\n",
    "            default_tables.append(table_name)\n",
    "        if idx < len_ and toks[idx] == \"on\":\n",
    "            idx += 1\n",
    "            idx, this_conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "            if len(conds) > 0:\n",
    "                conds.append('and')\n",
    "            conds.extend(this_conds)\n",
    "\n",
    "        if isBlock:\n",
    "            if idx < len_ and toks[idx] == ')':\n",
    "                idx += 1\n",
    "        if idx < len_ and (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "            break\n",
    "\n",
    "    return idx, table_units, conds, default_tables\n",
    "\n",
    "print(\"✓ Condition and FROM clause parsing loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ WHERE, GROUP BY, ORDER BY, HAVING, LIMIT parsing loaded\n"
     ]
    }
   ],
   "source": [
    "def parse_where(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    if idx >= len(toks) or toks[idx] != 'where':\n",
    "        return idx, []\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_group_by(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    col_units = []\n",
    "    if idx >= len(toks) or toks[idx] != 'group':\n",
    "        return idx, col_units\n",
    "    idx += 1\n",
    "    if idx < len(toks) and toks[idx] == 'by':\n",
    "        idx += 1\n",
    "    while idx < len(toks) and not (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "        idx, col_unit = parse_col_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        col_units.append(col_unit)\n",
    "        if idx < len(toks) and toks[idx] == ',':\n",
    "            idx += 1\n",
    "        else:\n",
    "            break\n",
    "    return idx, col_units\n",
    "\n",
    "\n",
    "def parse_order_by(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    val_units = []\n",
    "    order_type = 'asc'\n",
    "    if idx >= len(toks) or toks[idx] != 'order':\n",
    "        return idx, val_units\n",
    "    idx += 1\n",
    "    if idx < len(toks) and toks[idx] == 'by':\n",
    "        idx += 1\n",
    "    while idx < len(toks) and not (toks[idx] in CLAUSE_KEYWORDS or toks[idx] in (\")\", \";\")):\n",
    "        idx, val_unit = parse_val_unit(toks, idx, tables_with_alias, schema, default_tables)\n",
    "        val_units.append(val_unit)\n",
    "        if idx < len(toks) and toks[idx] in ORDER_OPS:\n",
    "            order_type = toks[idx]\n",
    "            idx += 1\n",
    "        if idx < len(toks) and toks[idx] == ',':\n",
    "            idx += 1\n",
    "        else:\n",
    "            break\n",
    "    return idx, (order_type, val_units)\n",
    "\n",
    "\n",
    "def parse_having(toks, start_idx, tables_with_alias, schema, default_tables):\n",
    "    idx = start_idx\n",
    "    if idx >= len(toks) or toks[idx] != 'having':\n",
    "        return idx, []\n",
    "    idx += 1\n",
    "    idx, conds = parse_condition(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    return idx, conds\n",
    "\n",
    "\n",
    "def parse_limit(toks, start_idx):\n",
    "    idx = start_idx\n",
    "    if idx < len(toks) and toks[idx] == 'limit':\n",
    "        idx += 1\n",
    "        if idx < len(toks):\n",
    "            try:\n",
    "                return idx + 1, int(toks[idx])\n",
    "            except:\n",
    "                return idx, None\n",
    "    return idx, None\n",
    "\n",
    "print(\"✓ WHERE, GROUP BY, ORDER BY, HAVING, LIMIT parsing loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main SQL parsing function loaded\n"
     ]
    }
   ],
   "source": [
    "def parse_sql(toks, start_idx, tables_with_alias, schema):\n",
    "    \"\"\"Main SQL parsing function\"\"\"\n",
    "    isBlock = False\n",
    "    idx = start_idx\n",
    "\n",
    "    sql = {}\n",
    "    if idx < len(toks) and toks[idx] == '(':\n",
    "        isBlock = True\n",
    "        idx += 1\n",
    "\n",
    "    # Parse FROM to get default tables\n",
    "    from_end_idx, table_units, conds, default_tables = parse_from(toks, start_idx, tables_with_alias, schema)\n",
    "    sql['from'] = {'table_units': table_units, 'conds': conds}\n",
    "    \n",
    "    # Parse SELECT\n",
    "    _, select_col_units = parse_select(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    idx = from_end_idx\n",
    "    sql['select'] = select_col_units\n",
    "    \n",
    "    # Parse WHERE\n",
    "    idx, where_conds = parse_where(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['where'] = where_conds\n",
    "    \n",
    "    # Parse GROUP BY\n",
    "    idx, group_col_units = parse_group_by(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['groupBy'] = group_col_units\n",
    "    \n",
    "    # Parse HAVING\n",
    "    idx, having_conds = parse_having(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['having'] = having_conds\n",
    "    \n",
    "    # Parse ORDER BY\n",
    "    idx, order_col_units = parse_order_by(toks, idx, tables_with_alias, schema, default_tables)\n",
    "    sql['orderBy'] = order_col_units\n",
    "    \n",
    "    # Parse LIMIT\n",
    "    idx, limit_val = parse_limit(toks, idx)\n",
    "    sql['limit'] = limit_val\n",
    "\n",
    "    idx = skip_semicolon(toks, idx)\n",
    "    if isBlock:\n",
    "        if idx < len(toks) and toks[idx] == ')':\n",
    "            idx += 1\n",
    "    idx = skip_semicolon(toks, idx)\n",
    "\n",
    "    # Parse INTERSECT/UNION/EXCEPT\n",
    "    for op in SQL_OPS:\n",
    "        sql[op] = None\n",
    "    if idx < len(toks) and toks[idx] in SQL_OPS:\n",
    "        sql_op = toks[idx]\n",
    "        idx += 1\n",
    "        idx, IUE_sql = parse_sql(toks, idx, tables_with_alias, schema)\n",
    "        sql[sql_op] = IUE_sql\n",
    "    return idx, sql\n",
    "\n",
    "\n",
    "def get_sql(schema, query):\n",
    "    \"\"\"Parse SQL query into structured representation\"\"\"\n",
    "    try:\n",
    "        toks = tokenize(query)\n",
    "        if not toks:\n",
    "            return None\n",
    "        tables_with_alias = get_tables_with_alias(schema.schema, toks)\n",
    "        _, sql = parse_sql(toks, 0, tables_with_alias, schema)\n",
    "        return sql\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"✓ Main SQL parsing function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spider evaluation functions loaded\n"
     ]
    }
   ],
   "source": [
    "def get_scores(count, pred_total, label_total):\n",
    "    if pred_total != label_total:\n",
    "        return 0, 0, 0\n",
    "    elif count == pred_total:\n",
    "        return 1, 1, 1\n",
    "    return 0, 0, 0\n",
    "\n",
    "\n",
    "def eval_sel(pred, label):\n",
    "    pred_sel = pred['select'][1]\n",
    "    label_sel = label['select'][1][:]\n",
    "    label_wo_agg = [unit[1] for unit in label_sel]\n",
    "    pred_total = len(pred_sel)\n",
    "    label_total = len(label_sel)\n",
    "    cnt = cnt_wo_agg = 0\n",
    "    \n",
    "    for unit in pred_sel:\n",
    "        if unit in label_sel:\n",
    "            cnt += 1\n",
    "            label_sel.remove(unit)\n",
    "        if unit[1] in label_wo_agg:\n",
    "            cnt_wo_agg += 1\n",
    "            label_wo_agg.remove(unit[1])\n",
    "    \n",
    "    return label_total, pred_total, cnt, cnt_wo_agg\n",
    "\n",
    "\n",
    "def eval_where(pred, label):\n",
    "    pred_conds = [unit for unit in pred['where'][::2]]\n",
    "    label_conds = [unit for unit in label['where'][::2]]\n",
    "    label_wo_agg = [unit[2] for unit in label_conds]\n",
    "    pred_total = len(pred_conds)\n",
    "    label_total = len(label_conds)\n",
    "    cnt = cnt_wo_agg = 0\n",
    "    \n",
    "    for unit in pred_conds:\n",
    "        if unit in label_conds:\n",
    "            cnt += 1\n",
    "            label_conds.remove(unit)\n",
    "        if len(unit) > 2 and unit[2] in label_wo_agg:\n",
    "            cnt_wo_agg += 1\n",
    "            label_wo_agg.remove(unit[2])\n",
    "    \n",
    "    return label_total, pred_total, cnt, cnt_wo_agg\n",
    "\n",
    "\n",
    "def eval_group(pred, label):\n",
    "    pred_cols = [str(unit[1]) for unit in pred['groupBy']]\n",
    "    label_cols = [str(unit[1]) for unit in label['groupBy']]\n",
    "    pred_total = len(pred_cols)\n",
    "    label_total = len(label_cols)\n",
    "    cnt = 0\n",
    "    \n",
    "    for col in pred_cols:\n",
    "        if col in label_cols:\n",
    "            cnt += 1\n",
    "            label_cols.remove(col)\n",
    "    \n",
    "    return label_total, pred_total, cnt\n",
    "\n",
    "\n",
    "def eval_having(pred, label):\n",
    "    pred_total = 1 if len(pred['having']) > 0 else 0\n",
    "    label_total = 1 if len(label['having']) > 0 else 0\n",
    "    cnt = 1 if pred['having'] == label['having'] else 0\n",
    "    return label_total, pred_total, cnt\n",
    "\n",
    "\n",
    "def eval_order(pred, label):\n",
    "    pred_total = 1 if len(pred['orderBy']) > 0 else 0\n",
    "    label_total = 1 if len(label['orderBy']) > 0 else 0\n",
    "    cnt = 1 if pred['orderBy'] == label['orderBy'] else 0\n",
    "    return label_total, pred_total, cnt\n",
    "\n",
    "\n",
    "def eval_and_or(pred, label):\n",
    "    pred_ao = set(pred['where'][1::2]) if len(pred['where']) > 1 else set()\n",
    "    label_ao = set(label['where'][1::2]) if len(label['where']) > 1 else set()\n",
    "    if pred_ao == label_ao:\n",
    "        return 1, 1, 1\n",
    "    return len(label_ao), len(pred_ao), 0\n",
    "\n",
    "\n",
    "def eval_IUEN(pred, label):\n",
    "    lt = pt = cnt = 0\n",
    "    for op in SQL_OPS:\n",
    "        if pred[op] is not None:\n",
    "            pt += 1\n",
    "        if label[op] is not None:\n",
    "            lt += 1\n",
    "        if pred[op] is not None and label[op] is not None:\n",
    "            cnt += 1\n",
    "    return lt, pt, cnt\n",
    "\n",
    "\n",
    "def get_keywords(sql):\n",
    "    res = set()\n",
    "    if len(sql['where']) > 0:\n",
    "        res.add('where')\n",
    "    if len(sql['groupBy']) > 0:\n",
    "        res.add('group')\n",
    "    if len(sql['having']) > 0:\n",
    "        res.add('having')\n",
    "    if len(sql['orderBy']) > 0:\n",
    "        res.add('order')\n",
    "    if sql['limit'] is not None:\n",
    "        res.add('limit')\n",
    "    for op in SQL_OPS:\n",
    "        if sql[op] is not None:\n",
    "            res.add(op)\n",
    "    return res\n",
    "\n",
    "\n",
    "def eval_keywords(pred, label):\n",
    "    pred_keywords = get_keywords(pred)\n",
    "    label_keywords = get_keywords(label)\n",
    "    pred_total = len(pred_keywords)\n",
    "    label_total = len(label_keywords)\n",
    "    cnt = len(pred_keywords & label_keywords)\n",
    "    return label_total, pred_total, cnt\n",
    "\n",
    "print(\"✓ Spider evaluation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluator class loaded\n"
     ]
    }
   ],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.partial_scores = None\n",
    "    \n",
    "    def eval_exact_match(self, pred, label):\n",
    "        partial_scores = self.eval_partial_match(pred, label)\n",
    "        self.partial_scores = partial_scores\n",
    "        \n",
    "        for _, score in partial_scores.items():\n",
    "            if score['f1'] != 1:\n",
    "                return 0\n",
    "        \n",
    "        if len(label['from']['table_units']) > 0:\n",
    "            label_tables = sorted([str(t) for t in label['from']['table_units']])\n",
    "            pred_tables = sorted([str(t) for t in pred['from']['table_units']])\n",
    "            return 1 if label_tables == pred_tables else 0\n",
    "        return 1\n",
    "    \n",
    "    def eval_partial_match(self, pred, label):\n",
    "        res = {}\n",
    "        \n",
    "        # SELECT\n",
    "        label_total, pred_total, cnt, cnt_wo_agg = eval_sel(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['select'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        acc, rec, f1 = get_scores(cnt_wo_agg, pred_total, label_total)\n",
    "        res['select(no AGG)'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # WHERE\n",
    "        label_total, pred_total, cnt, cnt_wo_agg = eval_where(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['where'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        acc, rec, f1 = get_scores(cnt_wo_agg, pred_total, label_total)\n",
    "        res['where(no OP)'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # GROUP BY\n",
    "        label_total, pred_total, cnt = eval_group(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['group(no Having)'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        label_total, pred_total, cnt = eval_having(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['group'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # ORDER BY\n",
    "        label_total, pred_total, cnt = eval_order(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['order'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # AND/OR\n",
    "        label_total, pred_total, cnt = eval_and_or(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['and/or'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # IUEN\n",
    "        label_total, pred_total, cnt = eval_IUEN(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['IUEN'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        # Keywords\n",
    "        label_total, pred_total, cnt = eval_keywords(pred, label)\n",
    "        acc, rec, f1 = get_scores(cnt, pred_total, label_total)\n",
    "        res['keywords'] = {'acc': acc, 'rec': rec, 'f1': f1, 'label_total': label_total, 'pred_total': pred_total}\n",
    "        \n",
    "        return res\n",
    "\n",
    "print(\"✓ Evaluator class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tolerant File Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tolerant file parser loaded\n"
     ]
    }
   ],
   "source": [
    "def _strip_outer_quotes(s: str):\n",
    "    s = s.strip()\n",
    "    if s.endswith(\",\"):\n",
    "        s = s[:-1].rstrip()\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        return s[1:-1]\n",
    "    return s\n",
    "\n",
    "\n",
    "def parse_file(filepath):\n",
    "    \"\"\"Tolerant parser for SQL prediction results\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split sections\n",
    "    m = re.search(r'^\\s*DATABASE\\s*:', text, flags=re.I|re.M)\n",
    "    if not m:\n",
    "        raise ValueError(\"Could not find 'DATABASE :' section\")\n",
    "    \n",
    "    rows_text = text[:m.start()].strip()\n",
    "    db_text = text[m.end():].strip()\n",
    "    \n",
    "    # Parse records line-by-line\n",
    "    records = []\n",
    "    current = None\n",
    "    \n",
    "    for line in rows_text.splitlines():\n",
    "        t = line.strip()\n",
    "        if t.startswith(\"{\"):\n",
    "            current = {\"query\": \"\", \"sql_truth\": \"\", \"sql_pred\": \"\"}\n",
    "        elif t.startswith(\"}\"):\n",
    "            if current and all(current[k] for k in [\"query\", \"sql_truth\", \"sql_pred\"]):\n",
    "                records.append(current)\n",
    "            current = None\n",
    "        elif current is not None:\n",
    "            m = re.match(r'^\\s*\"(?P<key>query|sql_truth|sql_pred)\"\\s*:\\s*(?P<val>.+)$', line)\n",
    "            if m:\n",
    "                current[m.group(\"key\")] = _strip_outer_quotes(m.group(\"val\"))\n",
    "    \n",
    "    # Parse database IDs\n",
    "    start = db_text.find('[')\n",
    "    end = db_text.rfind(']')\n",
    "    if start == -1 or end == -1:\n",
    "        raise ValueError(\"Could not find database list\")\n",
    "    \n",
    "    db_parts = [p.strip() for p in db_text[start+1:end].split(',')]\n",
    "    db_ids = []\n",
    "    for p in db_parts:\n",
    "        if p and len(p) >= 2 and p[0] in \"'\\\"\" and p[-1] in \"'\\\"\":\n",
    "            db_ids.append(p[1:-1])\n",
    "    \n",
    "    # Combine\n",
    "    n = min(len(records), len(db_ids))\n",
    "    for i in range(n):\n",
    "        records[i]['db_id'] = db_ids[i]\n",
    "    \n",
    "    return records[:n]\n",
    "\n",
    "print(\"✓ Tolerant file parser loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def normalize_sql(sql: str) -> str:\n",
    "    sql = \" \".join((sql or \"\").split()).strip()\n",
    "    if sql.endswith(\";\"):\n",
    "        sql = sql[:-1]\n",
    "    return sql.lower()\n",
    "\n",
    "\n",
    "def extract_keywords(sql: str) -> set:\n",
    "    tokens = re.findall(r\"[A-Za-z_]+\", (sql or \"\").lower())\n",
    "    return set(t for t in tokens if t in SQL_KEYWORDS)\n",
    "\n",
    "\n",
    "def find_db_path(db_id: str):\n",
    "    p1 = os.path.join(DB_DIR, db_id, f\"{db_id}.sqlite\")\n",
    "    p2 = os.path.join(DB_DIR, f\"{db_id}.sqlite\")\n",
    "    return p1 if os.path.exists(p1) else (p2 if os.path.exists(p2) else None)\n",
    "\n",
    "\n",
    "def exec_sql(db_path: str, sql: str):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        rows = cur.fetchall()\n",
    "        conn.close()\n",
    "        return True, rows\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def results_equal(a, b):\n",
    "    try:\n",
    "        return sorted(tuple(r) for r in a) == sorted(tuple(r) for r in b)\n",
    "    except:\n",
    "        return a == b\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 255 query pairs\n",
      "  Unique databases: 126\n",
      "\n",
      "  Sample query: How many heads of the departments are older than 56 ?...\n",
      "  Sample DB: department_management\n"
     ]
    }
   ],
   "source": [
    "# Load data using tolerant parser\n",
    "data = parse_file(FILE_PATH)\n",
    "print(f\"\\n✓ Loaded {len(data)} query pairs\")\n",
    "print(f\"  Unique databases: {len(set(r['db_id'] for r in data))}\")\n",
    "print(f\"\\n  Sample query: {data[0]['query'][:60]}...\")\n",
    "print(f\"  Sample DB: {data[0]['db_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING COMPLETE EVALUATION\n",
      "================================================================================\n",
      "Processed 50/255...\n",
      "Processed 100/255...\n",
      "Processed 150/255...\n",
      "Processed 200/255...\n",
      "Processed 250/255...\n",
      "\n",
      "✓ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNNING COMPLETE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total = len(data)\n",
    "\n",
    "# String-based metrics\n",
    "exact_match = 0\n",
    "kw_prec, kw_rec, kw_f1 = [], [], []\n",
    "\n",
    "# Execution metrics\n",
    "exec_match = 0\n",
    "exec_attempted = 0\n",
    "exec_pred_err = 0\n",
    "exec_gold_err = 0\n",
    "\n",
    "# Spider structural metrics\n",
    "evaluator = Evaluator()\n",
    "partial_types = ['select', 'select(no AGG)', 'where', 'where(no OP)', \n",
    "                 'group(no Having)', 'group', 'order', 'and/or', 'IUEN', 'keywords']\n",
    "scores = {'count': 0, 'exact': 0, 'partial': {}}\n",
    "for type_ in partial_types:\n",
    "    scores['partial'][type_] = {'acc': 0, 'rec': 0, 'f1': 0, 'acc_count': 0, 'rec_count': 0}\n",
    "\n",
    "parse_errors = 0\n",
    "mismatches = []\n",
    "\n",
    "for i, r in enumerate(data):\n",
    "    gold_str = normalize_sql(r.get(\"sql_truth\", \"\"))\n",
    "    pred_str = normalize_sql(r.get(\"sql_pred\", \"\"))\n",
    "    db_id = r.get(\"db_id\", \"\")\n",
    "    \n",
    "    # 1. Exact Match\n",
    "    if pred_str == gold_str:\n",
    "        exact_match += 1\n",
    "    else:\n",
    "        if len(mismatches) < SHOW_FIRST_N_MISMATCHES:\n",
    "            mismatches.append((i, db_id, r.get(\"query\"), gold_str, pred_str))\n",
    "    \n",
    "    # 2. Keyword Metrics\n",
    "    kg = extract_keywords(gold_str)\n",
    "    kp = extract_keywords(pred_str)\n",
    "    inter = len(kg & kp)\n",
    "    prec = inter / (len(kp) or 1)\n",
    "    rec = inter / (len(kg) or 1)\n",
    "    f1 = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    kw_prec.append(prec)\n",
    "    kw_rec.append(rec)\n",
    "    kw_f1.append(f1)\n",
    "    \n",
    "    # 3. Execution Match\n",
    "    if USE_EXEC:\n",
    "        path = find_db_path(db_id)\n",
    "        if path:\n",
    "            exec_attempted += 1\n",
    "            ok_g, res_g = exec_sql(path, r[\"sql_truth\"])\n",
    "            ok_p, res_p = exec_sql(path, r[\"sql_pred\"])\n",
    "            \n",
    "            if not ok_g:\n",
    "                exec_gold_err += 1\n",
    "            if not ok_p:\n",
    "                exec_pred_err += 1\n",
    "            \n",
    "            if ok_g and ok_p and results_equal(res_g, res_p):\n",
    "                exec_match += 1\n",
    "    \n",
    "    # 4. Spider Structural Evaluation\n",
    "    path = find_db_path(db_id)\n",
    "    if path:\n",
    "        try:\n",
    "            schema = Schema(get_schema(path))\n",
    "            g_sql = get_sql(schema, r[\"sql_truth\"])\n",
    "            p_sql = get_sql(schema, r[\"sql_pred\"])\n",
    "\n",
    "            if g_sql and p_sql:\n",
    "                scores['count'] += 1\n",
    "                \n",
    "                exact_score = evaluator.eval_exact_match(p_sql, g_sql)\n",
    "                partial_scores = evaluator.partial_scores\n",
    "                \n",
    "                scores['exact'] += exact_score\n",
    "                \n",
    "                for type_ in partial_types:\n",
    "                    if partial_scores[type_]['pred_total'] > 0:\n",
    "                        scores['partial'][type_]['acc'] += partial_scores[type_]['acc']\n",
    "                        scores['partial'][type_]['acc_count'] += 1\n",
    "                    if partial_scores[type_]['label_total'] > 0:\n",
    "                        scores['partial'][type_]['rec'] += partial_scores[type_]['rec']\n",
    "                        scores['partial'][type_]['rec_count'] += 1\n",
    "                    scores['partial'][type_]['f1'] += partial_scores[type_]['f1']\n",
    "            else:\n",
    "                parse_errors += 1\n",
    "        except Exception as e:\n",
    "            parse_errors += 1\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Processed {i + 1}/{total}...\")\n",
    "\n",
    "# Calculate averages for Spider metrics\n",
    "if scores['count'] > 0:\n",
    "    scores['exact'] /= scores['count']\n",
    "    for type_ in partial_types:\n",
    "        if scores['partial'][type_]['acc_count'] > 0:\n",
    "            scores['partial'][type_]['acc'] /= scores['partial'][type_]['acc_count']\n",
    "        if scores['partial'][type_]['rec_count'] > 0:\n",
    "            scores['partial'][type_]['rec'] /= scores['partial'][type_]['rec_count']\n",
    "        scores['partial'][type_]['f1'] /= scores['count']\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Part 1 - String Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 1: STRING-BASED METRICS\n",
      "================================================================================\n",
      "\n",
      "Metric                                   Score           Percentage\n",
      "--------------------------------------------------------------------------------\n",
      "Exact String Match                       51              20.00%\n",
      "Keyword Precision (avg)                  0.8696\n",
      "Keyword Recall (avg)                     0.8859\n",
      "Keyword F1 (avg)                         0.8693\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 1: STRING-BASED METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "em_pct = (exact_match / total * 100) if total > 0 else 0\n",
    "avg_prec = mean(kw_prec) if kw_prec else 0\n",
    "avg_rec = mean(kw_rec) if kw_rec else 0\n",
    "avg_f1 = mean(kw_f1) if kw_f1 else 0\n",
    "\n",
    "print(f\"\\n{'Metric':<40} {'Score':<15} {'Percentage'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Exact String Match':<40} {exact_match:<15} {em_pct:.2f}%\")\n",
    "print(f\"{'Keyword Precision (avg)':<40} {avg_prec:.4f}\")\n",
    "print(f\"{'Keyword Recall (avg)':<40} {avg_rec:.4f}\")\n",
    "print(f\"{'Keyword F1 (avg)':<40} {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Part 2 - Execution Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: EXECUTION ACCURACY\n",
      "================================================================================\n",
      "\n",
      "Queries Executed: 255\n",
      "Execution Match:  189 (74.12%)\n",
      "Prediction Error: 1 (0.39%)\n",
      "Gold Error:       1 (0.39%)\n",
      "\n",
      "Effective Accuracy (valid executions only): 189/253 (74.70%)\n"
     ]
    }
   ],
   "source": [
    "if USE_EXEC:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PART 2: EXECUTION ACCURACY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    exec_pct = (exec_match / exec_attempted * 100) if exec_attempted > 0 else 0\n",
    "    \n",
    "    print(f\"\\nQueries Executed: {exec_attempted}\")\n",
    "    print(f\"Execution Match:  {exec_match} ({exec_pct:.2f}%)\")\n",
    "    print(f\"Prediction Error: {exec_pred_err} ({exec_pred_err/exec_attempted*100:.2f}%)\")\n",
    "    print(f\"Gold Error:       {exec_gold_err} ({exec_gold_err/exec_attempted*100:.2f}%)\")\n",
    "    \n",
    "    valid = exec_attempted - exec_pred_err - exec_gold_err\n",
    "    if valid > 0:\n",
    "        eff_acc = (exec_match / valid * 100)\n",
    "        print(f\"\\nEffective Accuracy (valid executions only): {exec_match}/{valid} ({eff_acc:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Part 3 - Spider Component Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: SPIDER-STYLE STRUCTURAL COMPONENT MATCHING\n",
      "================================================================================\n",
      "\n",
      "Successfully Parsed: 151/255\n",
      "Parsing Errors: 104\n",
      "\n",
      "====================== EXACT MATCHING ACCURACY =====================\n",
      "exact match          0.000               \n",
      "\n",
      "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
      "select               0.709               \n",
      "select(no AGG)       0.709               \n",
      "where                0.522               \n",
      "where(no OP)         0.687               \n",
      "group(no Having)     0.880               \n",
      "group                0.167               \n",
      "order                0.800               \n",
      "and/or               1.000               \n",
      "IUEN                 1.000               \n",
      "keywords             0.889               \n",
      "\n",
      "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
      "select               0.709               \n",
      "select(no AGG)       0.709               \n",
      "where                0.636               \n",
      "where(no OP)         0.836               \n",
      "group(no Having)     0.917               \n",
      "group                0.167               \n",
      "order                0.667               \n",
      "and/or               0.974               \n",
      "IUEN                 0.333               \n",
      "keywords             0.897               \n",
      "\n",
      "---------------------- PARTIAL MATCHING F1 --------------------------\n",
      "select               0.709               \n",
      "select(no AGG)       0.709               \n",
      "where                0.788               \n",
      "where(no OP)         0.861               \n",
      "group(no Having)     0.980               \n",
      "group                0.007               \n",
      "order                0.212               \n",
      "and/or               0.974               \n",
      "IUEN                 0.987               \n",
      "keywords             0.907               \n"
     ]
    }
   ],
   "source": [
    "if scores['count'] > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PART 3: SPIDER-STYLE STRUCTURAL COMPONENT MATCHING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nSuccessfully Parsed: {scores['count']}/{total}\")\n",
    "    print(f\"Parsing Errors: {parse_errors}\")\n",
    "    \n",
    "    print('\\n====================== EXACT MATCHING ACCURACY =====================')\n",
    "    print(f\"{'exact match':<20} {scores['exact']:<20.3f}\")\n",
    "    \n",
    "    print('\\n---------------------PARTIAL MATCHING ACCURACY----------------------')\n",
    "    for type_ in partial_types:\n",
    "        print(f\"{type_:<20} {scores['partial'][type_]['acc']:<20.3f}\")\n",
    "    \n",
    "    print('\\n---------------------- PARTIAL MATCHING RECALL ----------------------')\n",
    "    for type_ in partial_types:\n",
    "        print(f\"{type_:<20} {scores['partial'][type_]['rec']:<20.3f}\")\n",
    "    \n",
    "    print('\\n---------------------- PARTIAL MATCHING F1 --------------------------')\n",
    "    for type_ in partial_types:\n",
    "        print(f\"{type_:<20} {scores['partial'][type_]['f1']:<20.3f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Spider evaluation skipped (parsing errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error diagonostic errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARSING ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "                     Error Category  Count Percentage\n",
      "                Successfully Parsed    151     59.22%\n",
      "  Parse Pred Failed (returned None)    104     40.78%\n",
      "Tokenize Gold Failed (empty tokens)      0      0.00%\n",
      "                 Database Not Found      0      0.00%\n",
      "  Parse Gold Failed (returned None)      0      0.00%\n",
      "Tokenize Pred Failed (empty tokens)      0      0.00%\n",
      "                       Schema Error      0      0.00%\n",
      "                    Assertion Error      0      0.00%\n",
      "                          Key Error      0      0.00%\n",
      "                       Other Errors      0      0.00%\n",
      "\n",
      "================================================================================\n",
      "DETAILED BREAKDOWN (First 3 examples of each error type)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PARSE PRED FAILED (RETURNED NONE) (104 total)\n",
      "================================================================================\n",
      "\n",
      "  Index: 1\n",
      "  DB: farm\n",
      "  Query: What are the hosts of competitions whose theme is not \"Alien...\n",
      "  SQL: SELECT DISTINCT c.official_name FROM farm_competition fc JOIN city c ON fc.host_...\n",
      "\n",
      "  Index: 2\n",
      "  DB: farm\n",
      "  Query: Please show the themes of competitions with host cities havi...\n",
      "  SQL: SELECT fc.theme FROM farm_competition fc JOIN city c ON fc.host_city_id = c.city...\n",
      "\n",
      "  Index: 5\n",
      "  DB: bike_1\n",
      "  Query: What is the latitude, longitude, and city of the station fro...\n",
      "  SQL: SELECT s.lat, s.long, s.city FROM station s JOIN trip t ON s.id = t.start_statio...\n",
      "\n",
      "================================================================================\n",
      "EXPORTING DETAILED RESULTS\n",
      "================================================================================\n",
      "Exported 255 records to 'parsing_errors_detailed.csv'\n",
      "\n",
      "Preview of export:\n",
      "                         Category  index                       db_id                                                        query                                                                              sql\n",
      "Parse Pred Failed (returned None)      1                        farm What are the hosts of competitions whose theme is not \"Alien SELECT DISTINCT c.official_name FROM farm_competition fc JOIN city c ON fc.host_\n",
      "Parse Pred Failed (returned None)      2                        farm Please show the themes of competitions with host cities havi SELECT fc.theme FROM farm_competition fc JOIN city c ON fc.host_city_id = c.city\n",
      "Parse Pred Failed (returned None)      5                      bike_1 What is the latitude, longitude, and city of the station fro SELECT s.lat, s.long, s.city FROM station s JOIN trip t ON s.id = t.start_statio\n",
      "Parse Pred Failed (returned None)      6                      bike_1 Which days had a minimum dew point smaller than any day in z SELECT w.zip_code  FROM weather w  WHERE w.min_dew_point_f < (SELECT MIN(w2.min_\n",
      "Parse Pred Failed (returned None)     14             product_catalog Find the level name of the catalog with the lowest price (in SELECT cs.catalog_level_name FROM Catalog_Contents cc JOIN Catalog_Structure cs \n",
      "Parse Pred Failed (returned None)     20                   allergy_1 What are the last names and ages of the students who are all SELECT s.LName, s.Age FROM Student s JOIN Has_Allergy ha ON s.StuID = ha.StuID W\n",
      "Parse Pred Failed (returned None)     23           journal_committee Show the names and ages of editors and the theme of journals SELECT e.name, e.age, j.theme FROM editor e JOIN journal_committee jc ON e.edito\n",
      "Parse Pred Failed (returned None)     26 customers_card_transactions What are the different card types, and how many transactions SELECT card_type_code , COUNT(ft.transaction_id) AS transaction_count  FROM cust\n",
      "Parse Pred Failed (returned None)     28                  race_track What are the names of different tracks, and how many races h SELECT t.name, COUNT(r.race_id) AS race_count FROM track t LEFT JOIN race r ON t\n",
      "Parse Pred Failed (returned None)     30              insurance_fnol Which customers do not have a first notification of loss rec SELECT c.Customer_name FROM Customers c LEFT JOIN First_Notification_of_Loss fno\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARSING ERROR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track all error types\n",
    "error_data = {\n",
    "    'Database Not Found': [],\n",
    "    'Tokenize Gold Failed (empty tokens)': [],\n",
    "    'Tokenize Pred Failed (empty tokens)': [],\n",
    "    'Parse Gold Failed (returned None)': [],\n",
    "    'Parse Pred Failed (returned None)': [],\n",
    "    'Schema Error': [],\n",
    "    'Assertion Error': [],\n",
    "    'Key Error': [],\n",
    "    'Other Errors': [],\n",
    "    'Successfully Parsed': []\n",
    "}\n",
    "\n",
    "for i, r in enumerate(data):\n",
    "    db_id = r.get(\"db_id\", \"\")\n",
    "    path = find_db_path(db_id)\n",
    "    \n",
    "    if not path:\n",
    "        error_data['Database Not Found'].append({\n",
    "            'index': i,\n",
    "            'db_id': db_id,\n",
    "            'query': r['query'][:60],\n",
    "            'error': 'DB file not found'\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Tokenization\n",
    "        gold_toks = tokenize(r[\"sql_truth\"])\n",
    "        pred_toks = tokenize(r[\"sql_pred\"])\n",
    "        \n",
    "        if not gold_toks:\n",
    "            error_data['Tokenize Gold Failed (empty tokens)'].append({\n",
    "                'index': i,\n",
    "                'db_id': db_id,\n",
    "                'query': r['query'][:60],\n",
    "                'sql': r[\"sql_truth\"][:80]\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        if not pred_toks:\n",
    "            error_data['Tokenize Pred Failed (empty tokens)'].append({\n",
    "                'index': i,\n",
    "                'db_id': db_id,\n",
    "                'query': r['query'][:60],\n",
    "                'sql': r[\"sql_pred\"][:80]\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Parsing\n",
    "        schema = Schema(get_schema(path))\n",
    "        g_sql = get_sql(schema, r[\"sql_truth\"])\n",
    "        p_sql = get_sql(schema, r[\"sql_pred\"])\n",
    "        \n",
    "        if g_sql is None:\n",
    "            error_data['Parse Gold Failed (returned None)'].append({\n",
    "                'index': i,\n",
    "                'db_id': db_id,\n",
    "                'query': r['query'][:60],\n",
    "                'sql': r[\"sql_truth\"][:80]\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        if p_sql is None:\n",
    "            error_data['Parse Pred Failed (returned None)'].append({\n",
    "                'index': i,\n",
    "                'db_id': db_id,\n",
    "                'query': r['query'][:60],\n",
    "                'sql': r[\"sql_pred\"][:80]\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Success\n",
    "        error_data['Successfully Parsed'].append({\n",
    "            'index': i,\n",
    "            'db_id': db_id,\n",
    "            'query': r['query'][:60]\n",
    "        })\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        error_data['Assertion Error'].append({\n",
    "            'index': i,\n",
    "            'db_id': db_id,\n",
    "            'query': r['query'][:60],\n",
    "            'error': str(e)[:60]\n",
    "        })\n",
    "    except KeyError as e:\n",
    "        error_data['Key Error'].append({\n",
    "            'index': i,\n",
    "            'db_id': db_id,\n",
    "            'query': r['query'][:60],\n",
    "            'error': f\"Missing: {e}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        error_data['Other Errors'].append({\n",
    "            'index': i,\n",
    "            'db_id': db_id,\n",
    "            'query': r['query'][:60],\n",
    "            'error': f\"{type(e).__name__}: {str(e)[:40]}\"\n",
    "        })\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for category, errors in error_data.items():\n",
    "    summary_data.append({\n",
    "        'Error Category': category,\n",
    "        'Count': len(errors),\n",
    "        'Percentage': f\"{len(errors)/len(data)*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Detailed breakdown of each error type\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED BREAKDOWN (First 3 examples of each error type)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for category, errors in error_data.items():\n",
    "    if errors and category != 'Successfully Parsed':\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"{category.upper()} ({len(errors)} total)\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Show first 3 examples\n",
    "        for example in errors[:3]:\n",
    "            print(f\"\\n  Index: {example['index']}\")\n",
    "            print(f\"  DB: {example['db_id']}\")\n",
    "            print(f\"  Query: {example['query']}...\")\n",
    "            if 'sql' in example:\n",
    "                print(f\"  SQL: {example['sql']}...\")\n",
    "            if 'error' in example:\n",
    "                print(f\"  Error: {example['error']}\")\n",
    "\n",
    "# Export to CSV (optional)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTING DETAILED RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_errors = []\n",
    "for category, errors in error_data.items():\n",
    "    for error in errors:\n",
    "        error['Category'] = category\n",
    "        all_errors.append(error)\n",
    "\n",
    "if all_errors:\n",
    "    error_df = pd.DataFrame(all_errors)\n",
    "    error_df = error_df[['Category', 'index', 'db_id', 'query'] + \n",
    "                        [col for col in error_df.columns if col not in ['Category', 'index', 'db_id', 'query']]]\n",
    "    error_df.to_csv('parsing_errors_detailed.csv', index=False)\n",
    "    print(f\"Exported {len(all_errors)} records to 'parsing_errors_detailed.csv'\")\n",
    "    \n",
    "    # Show preview\n",
    "    print(\"\\nPreview of export:\")\n",
    "    print(error_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYZING FAILED PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Found 104 cases where gold parses but pred fails\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Example 1 (Query #1)\n",
      "================================================================================\n",
      "Gold SQL (8 tokens):\n",
      "  SELECT Hosts FROM farm_competition WHERE Theme !=  'Aliens'\n",
      "\n",
      "Pred SQL (18 tokens):\n",
      "  SELECT DISTINCT c.official_name FROM farm_competition fc JOIN city c ON fc.host_city_id = c.city_id WHERE fc.theme <> 'Aliens'\n",
      "\n",
      "Potential issues: Pred much longer than gold\n",
      "\n",
      "================================================================================\n",
      "Example 2 (Query #2)\n",
      "================================================================================\n",
      "Gold SQL (18 tokens):\n",
      "  SELECT T2.Theme FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID  =  T2.Host_city_ID WHERE T1.Population  >  1000\n",
      "\n",
      "Pred SQL (16 tokens):\n",
      "  SELECT fc.theme FROM farm_competition fc JOIN city c ON fc.host_city_id = c.city_id WHERE c.population > 1000\n",
      "\n",
      "================================================================================\n",
      "Example 3 (Query #5)\n",
      "================================================================================\n",
      "Gold SQL (23 tokens):\n",
      "  SELECT T1.lat ,  T1.long ,  T1.city FROM station AS T1 JOIN trip AS T2 ON T1.id  =  T2.start_station_id ORDER BY T2.duration LIMIT 1\n",
      "\n",
      "Pred SQL (22 tokens):\n",
      "  SELECT s.lat, s.long, s.city FROM station s JOIN trip t ON s.id = t.start_station_id ORDER BY t.duration ASC LIMIT 1\n",
      "\n",
      "================================================================================\n",
      "Example 4 (Query #6)\n",
      "================================================================================\n",
      "Gold SQL (22 tokens):\n",
      "  SELECT date ,  zip_code FROM weather WHERE min_dew_point_f  <  (SELECT min(min_dew_point_f) FROM weather WHERE zip_code  =  94107)\n",
      "\n",
      "Pred SQL (22 tokens):\n",
      "  SELECT w.zip_code  FROM weather w  WHERE w.min_dew_point_f < (SELECT MIN(w2.min_dew_point_f) FROM weather w2 WHERE w2.zip_code = '94107')\n",
      "\n",
      "================================================================================\n",
      "Example 5 (Query #14)\n",
      "================================================================================\n",
      "Gold SQL (19 tokens):\n",
      "  SELECT t2.catalog_level_name FROM catalog_contents AS t1 JOIN catalog_structure AS t2 ON t1.catalog_level_number  =  t2.catalog_level_number ORDER BY t1.price_in_dollars LIMIT 1\n",
      "\n",
      "Pred SQL (25 tokens):\n",
      "  SELECT cs.catalog_level_name FROM Catalog_Contents cc JOIN Catalog_Structure cs ON cc.catalog_level_number = cs.catalog_level_number WHERE cc.price_in_dollars = (SELECT MIN(price_in_dollars) FROM Catalog_Contents);\n",
      "\n",
      "================================================================================\n",
      "Example 6 (Query #20)\n",
      "================================================================================\n",
      "Gold SQL (28 tokens):\n",
      "  SELECT lname ,  age FROM Student WHERE StuID IN (SELECT StuID FROM Has_allergy WHERE Allergy  =  \"Milk\" INTERSECT SELECT StuID FROM Has_allergy WHERE Allergy  =  \"Cat\")\n",
      "\n",
      "Pred SQL (35 tokens):\n",
      "  SELECT s.LName, s.Age FROM Student s JOIN Has_Allergy ha ON s.StuID = ha.StuID WHERE ha.Allergy IN ('milk', 'cat') GROUP BY s.LName, s.Age HAVING COUNT(DISTINCT ha.Allergy) = 2\n",
      "\n",
      "================================================================================\n",
      "Example 7 (Query #23)\n",
      "================================================================================\n",
      "Gold SQL (30 tokens):\n",
      "  SELECT T2.Name ,  T2.age ,  T3.Theme FROM journal_committee AS T1 JOIN editor AS T2 ON T1.Editor_ID  =  T2.Editor_ID JOIN journal AS T3 ON T1.Journal_ID  =  T3.Journal_ID ORDER BY T3.Theme ASC\n",
      "\n",
      "Pred SQL (27 tokens):\n",
      "  SELECT e.name, e.age, j.theme FROM editor e JOIN journal_committee jc ON e.editor_id = jc.editor_id JOIN journal j ON jc.journal_id = j.journal_id ORDER BY j.theme ASC\n",
      "\n",
      "================================================================================\n",
      "Example 8 (Query #26)\n",
      "================================================================================\n",
      "Gold SQL (22 tokens):\n",
      "  SELECT T2.card_type_code ,  count(*) FROM Financial_transactions AS T1 JOIN Customers_cards AS T2 ON T1.card_id  =  T2.card_id GROUP BY T2.card_type_code\n",
      "\n",
      "Pred SQL (23 tokens):\n",
      "  SELECT card_type_code , COUNT(ft.transaction_id) AS transaction_count  FROM customers_cards cc  LEFT JOIN financial_transactions ft ON cc.card_id = ft.card_id  GROUP BY card_type_code\n",
      "\n",
      "================================================================================\n",
      "Example 9 (Query #28)\n",
      "================================================================================\n",
      "Gold SQL (22 tokens):\n",
      "  SELECT T2.name ,  count(*) FROM race AS T1 JOIN track AS T2 ON T1.track_id  =  T2.track_id GROUP BY T1.track_id\n",
      "\n",
      "Pred SQL (23 tokens):\n",
      "  SELECT t.name, COUNT(r.race_id) AS race_count FROM track t LEFT JOIN race r ON t.track_id = r.track_id GROUP BY t.name\n",
      "\n",
      "================================================================================\n",
      "Example 10 (Query #30)\n",
      "================================================================================\n",
      "Gold SQL (19 tokens):\n",
      "  SELECT customer_name FROM customers EXCEPT SELECT t1.customer_name FROM customers AS t1 JOIN first_notification_of_loss AS t2 ON t1.customer_id  =  t2.customer_id\n",
      "\n",
      "Pred SQL (17 tokens):\n",
      "  SELECT c.Customer_name FROM Customers c LEFT JOIN First_Notification_of_Loss fnol ON c.Customer_ID = fnol.Customer_ID WHERE fnol.FNOL_ID IS NULL\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Compare gold vs pred for failed parses\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING FAILED PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "failed_examples = []\n",
    "\n",
    "for i, r in enumerate(data):\n",
    "    db_id = r.get(\"db_id\", \"\")\n",
    "    path = find_db_path(db_id)\n",
    "    \n",
    "    if not path:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        schema = Schema(get_schema(path))\n",
    "        g_sql = get_sql(schema, r[\"sql_truth\"])\n",
    "        p_sql = get_sql(schema, r[\"sql_pred\"])\n",
    "        \n",
    "        # Gold parses but pred doesn't\n",
    "        if g_sql is not None and p_sql is None:\n",
    "            failed_examples.append({\n",
    "                'index': i,\n",
    "                'gold': r[\"sql_truth\"],\n",
    "                'pred': r[\"sql_pred\"],\n",
    "                'gold_tokens': len(tokenize(r[\"sql_truth\"])),\n",
    "                'pred_tokens': len(tokenize(r[\"sql_pred\"]))\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nFound {len(failed_examples)} cases where gold parses but pred fails\\n\")\n",
    "\n",
    "# Show first 10 examples\n",
    "for idx, example in enumerate(failed_examples[:10]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx+1} (Query #{example['index']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Gold SQL ({example['gold_tokens']} tokens):\")\n",
    "    print(f\"  {example['gold']}\")\n",
    "    print(f\"\\nPred SQL ({example['pred_tokens']} tokens):\")\n",
    "    print(f\"  {example['pred']}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    issues = []\n",
    "    if len(example['pred']) > len(example['gold']) + 50:\n",
    "        issues.append(\"Pred much longer than gold\")\n",
    "    if example['pred'].count('(') != example['pred'].count(')'):\n",
    "        issues.append(\"Unbalanced parentheses\")\n",
    "    if '...' in example['pred'] or example['pred'].endswith('...'):\n",
    "        issues.append(\"Truncated SQL (contains ...)\")\n",
    "    if '\\n' in example['pred']:\n",
    "        issues.append(\"Contains newlines\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"\\nPotential issues: {', '.join(issues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "1. EXECUTION ACCURACY (Most Important - Does it work?)\n",
      "   → 74.12% of queries return correct results\n",
      "   This is your PRIMARY metric - queries that work correctly.\n",
      "\n",
      "2. STRING MATCHING (Least Important - Exact text match)\n",
      "   → 20.00% exact string matches\n",
      "   Low scores expected due to stylistic differences (aliases, spacing, etc.)\n",
      "\n",
      "3. SPIDER STRUCTURAL (Medium - Component matching)\n",
      "   → Evaluated on 151/255 queries (59.2%)\n",
      "   → 0.00% exact structural match\n",
      "   Parser is strict about syntax style, so some valid queries fail to parse.\n",
      "   This metric works best when gold and pred use similar SQL dialects.\n",
      "\n",
      "RECOMMENDATION:\n",
      "Use EXECUTION ACCURACY as your primary evaluation metric.\n",
      "Spider metrics are supplementary and only apply to parseable queries.\n"
     ]
    }
   ],
   "source": [
    "# Update your summary to emphasize this:\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. EXECUTION ACCURACY (Most Important - Does it work?)\")\n",
    "print(f\"   → {exec_pct:.2f}% of queries return correct results\")\n",
    "print(f\"   This is your PRIMARY metric - queries that work correctly.\\n\")\n",
    "\n",
    "print(f\"2. STRING MATCHING (Least Important - Exact text match)\")\n",
    "print(f\"   → {em_pct:.2f}% exact string matches\")\n",
    "print(f\"   Low scores expected due to stylistic differences (aliases, spacing, etc.)\\n\")\n",
    "\n",
    "print(f\"3. SPIDER STRUCTURAL (Medium - Component matching)\")\n",
    "print(f\"   → Evaluated on {scores['count']}/{total} queries ({scores['count']/total*100:.1f}%)\")\n",
    "print(f\"   → {scores['exact']*100:.2f}% exact structural match\")\n",
    "print(f\"   Parser is strict about syntax style, so some valid queries fail to parse.\")\n",
    "print(f\"   This metric works best when gold and pred use similar SQL dialects.\\n\")\n",
    "\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"Use EXECUTION ACCURACY as your primary evaluation metric.\")\n",
    "print(\"Spider metrics are supplementary and only apply to parseable queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPIDER PARTIAL COMPONENT SCORES\n",
      "================================================================================\n",
      "select                    F1: 0.709  |  Acc: 0.709  |  Rec: 0.709\n",
      "select(no AGG)            F1: 0.709  |  Acc: 0.709  |  Rec: 0.709\n",
      "where                     F1: 0.788  |  Acc: 0.522  |  Rec: 0.636\n",
      "where(no OP)              F1: 0.861  |  Acc: 0.687  |  Rec: 0.836\n",
      "group(no Having)          F1: 0.980  |  Acc: 0.880  |  Rec: 0.917\n",
      "group                     F1: 0.007  |  Acc: 0.167  |  Rec: 0.167\n",
      "order                     F1: 0.212  |  Acc: 0.800  |  Rec: 0.667\n",
      "and/or                    F1: 0.974  |  Acc: 1.000  |  Rec: 0.974\n",
      "IUEN                      F1: 0.987  |  Acc: 1.000  |  Rec: 0.333\n",
      "keywords                  F1: 0.907  |  Acc: 0.889  |  Rec: 0.897\n"
     ]
    }
   ],
   "source": [
    "if scores['count'] > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SPIDER PARTIAL COMPONENT SCORES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for type_ in partial_types:\n",
    "        f1 = scores['partial'][type_]['f1']\n",
    "        acc = scores['partial'][type_]['acc']\n",
    "        rec = scores['partial'][type_]['rec']\n",
    "        print(f\"{type_:<25} F1: {f1:.3f}  |  Acc: {acc:.3f}  |  Rec: {rec:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Analysis\n",
    "\n",
    "### Strong Performance `(F1 > 0.85)`:\n",
    "\n",
    "- **GROUP BY columns**: 0.980 - Nearly perfect at identifying what to group by\n",
    "- **IUEN (UNION/INTERSECT/EXCEPT)**: 0.987 - Excellent at set operations\n",
    "- **AND/OR logic**: 0.974 - Great at logical operators\n",
    "- **Keywords**: 0.907 - Uses correct SQL clauses\n",
    "- **WHERE (no operators)**: 0.861 - Good at identifying which columns to filter\n",
    "\n",
    "### Critical Weaknesses:\n",
    "\n",
    "#### `HAVING` clauses: `0.007 F1` - Almost complete failure\n",
    "\n",
    "The model groups correctly (0.980) but can't filter grouped results\n",
    "This is a significant gap in SQL generation capability\n",
    "\n",
    "\n",
    "#### `ORDER BY`: `0.212 F1` - Poor performance\n",
    "\n",
    "Low accuracy (0.800) but terrible recall (0.667)\n",
    "Model often omits ORDER BY when needed\n",
    "\n",
    "\n",
    "#### `WHERE` operators: `0.788 F1` - Decent but room for improvement\n",
    "\n",
    "Identifies the right columns (`0.861`) but struggles with exact operator matching\n",
    "\n",
    "\n",
    "\n",
    "### Actionable Insights:\n",
    "The `74%` execution accuracy is solid, but these component scores show where to improve:\n",
    "\n",
    "- **Priority fix**: Train on more HAVING clause examples\n",
    "- **Secondary fix**: Improve ORDER BY generation (especially knowing when it's required)\n",
    "- **Fine-tuning**: Better operator selection in WHERE clauses (=, !=, <>, >, <, etc.)\n",
    "\n",
    "The model understands SQL structure well (high keyword/logic scores) but needs refinement on advanced filtering and sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE MISMATCHES (First 10)\n",
      "================================================================================\n",
      "\n",
      "[1] Query #1 | DB: farm\n",
      "Q: What are the hosts of competitions whose theme is not \"Aliens\"?...\n",
      "G: select hosts from farm_competition where theme != 'aliens'...\n",
      "P: select distinct c.official_name from farm_competition fc join city c on fc.host_city_id = ...\n",
      "Extra keywords: {'distinct', 'on', 'join'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Query #2 | DB: farm\n",
      "Q: Please show the themes of competitions with host cities having populat...\n",
      "G: select t2.theme from city as t1 join farm_competition as t2 on t1.city_id = t2.host_city_i...\n",
      "P: select fc.theme from farm_competition fc join city c on fc.host_city_id = c.city_id where ...\n",
      "Missing keywords: {'as'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Query #3 | DB: student_assessment\n",
      "Q: What are the ids of the students who either registered or attended a c...\n",
      "G: select student_id from student_course_registrations union select student_id from student_c...\n",
      "P: select distinct student_id from student_course_registrations union select distinct student...\n",
      "Extra keywords: {'distinct'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] Query #4 | DB: bike_1\n",
      "Q: What is the id of the shortest trip?...\n",
      "G: select id from trip order by duration limit 1...\n",
      "P: select id from trip order by duration asc limit 1...\n",
      "Extra keywords: {'asc'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] Query #5 | DB: bike_1\n",
      "Q: What is the latitude, longitude, and city of the station from which th...\n",
      "G: select t1.lat , t1.long , t1.city from station as t1 join trip as t2 on t1.id = t2.start_s...\n",
      "P: select s.lat, s.long, s.city from station s join trip t on s.id = t.start_station_id order...\n",
      "Missing keywords: {'as'}\n",
      "Extra keywords: {'asc'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] Query #6 | DB: bike_1\n",
      "Q: Which days had a minimum dew point smaller than any day in zip code 94...\n",
      "G: select date , zip_code from weather where min_dew_point_f < (select min(min_dew_point_f) f...\n",
      "P: select w.zip_code from weather w where w.min_dew_point_f < (select min(w2.min_dew_point_f)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[7] Query #7 | DB: bike_1\n",
      "Q: What are the ids and names of all start stations that were the beginni...\n",
      "G: select start_station_id , start_station_name from trip group by start_station_name having ...\n",
      "P: select start_station_id , start_station_name from trip group by start_station_id , start_s...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[8] Query #9 | DB: book_2\n",
      "Q: List the publisher of the publication with the highest price....\n",
      "G: select publisher from publication order by price desc limit 1...\n",
      "P: select publisher from publication where price = (select max(price) from publication)...\n",
      "Missing keywords: {'by', 'desc', 'order', 'limit'}\n",
      "Extra keywords: {'max', 'where'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[9] Query #10 | DB: musical\n",
      "Q: List the name of actors whose age is not 20....\n",
      "G: select name from actor where age != 20...\n",
      "P: select name from actor where age <> 20...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[10] Query #11 | DB: musical\n",
      "Q: Show different nominees and the number of musicals they have been nomi...\n",
      "G: select nominee , count(*) from musical group by nominee...\n",
      "P: select nominee , count(musical_id) as number_of_musicals from musical group by nominee...\n",
      "Extra keywords: {'as'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if mismatches:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SAMPLE MISMATCHES (First {len(mismatches)})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, (i, db_id, query, gold, pred) in enumerate(mismatches, 1):\n",
    "        print(f\"\\n[{idx}] Query #{i} | DB: {db_id}\")\n",
    "        print(f\"Q: {query[:70]}...\")\n",
    "        print(f\"G: {gold[:90]}...\")\n",
    "        print(f\"P: {pred[:90]}...\")\n",
    "        \n",
    "        kg = extract_keywords(gold)\n",
    "        kp = extract_keywords(pred)\n",
    "        if kg - kp:\n",
    "            print(f\"Missing keywords: {kg - kp}\")\n",
    "        if kp - kg:\n",
    "            print(f\"Extra keywords: {kp - kg}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"\\n✓ All queries matched exactly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Dataset: 255 queries across 126 databases\n",
      "\n",
      "1. STRING MATCHING:\n",
      "   Exact Match:      20.00%\n",
      "   Keyword F1:      0.8693\n",
      "\n",
      "2. EXECUTION:\n",
      "   Execution Match:  74.12%\n",
      "   Effective (valid only):  74.70%\n",
      "\n",
      "3. SPIDER STRUCTURAL:\n",
      "   Exact Match:       0.00%\n",
      "   SELECT F1:        70.86%\n",
      "   WHERE F1:         78.81%\n",
      "   GROUP BY F1:       0.66%\n",
      "   ORDER BY F1:      21.19%\n",
      "\n",
      "================================================================================\n",
      "✓ EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset: {total} queries across {len(set(r['db_id'] for r in data))} databases\")\n",
    "\n",
    "print(f\"\\n1. STRING MATCHING:\")\n",
    "print(f\"   Exact Match:     {em_pct:>6.2f}%\")\n",
    "print(f\"   Keyword F1:      {avg_f1:>6.4f}\")\n",
    "\n",
    "if USE_EXEC:\n",
    "    print(f\"\\n2. EXECUTION:\")\n",
    "    print(f\"   Execution Match: {exec_pct:>6.2f}%\")\n",
    "    if valid > 0:\n",
    "        print(f\"   Effective (valid only): {eff_acc:>6.2f}%\")\n",
    "\n",
    "if scores['count'] > 0:\n",
    "    print(f\"\\n3. SPIDER STRUCTURAL:\")\n",
    "    print(f\"   Exact Match:     {scores['exact']*100:>6.2f}%\")\n",
    "    print(f\"   SELECT F1:       {scores['partial']['select']['f1']*100:>6.2f}%\")\n",
    "    print(f\"   WHERE F1:        {scores['partial']['where']['f1']*100:>6.2f}%\")\n",
    "    print(f\"   GROUP BY F1:     {scores['partial']['group']['f1']*100:>6.2f}%\")\n",
    "    print(f\"   ORDER BY F1:     {scores['partial']['order']['f1']*100:>6.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
